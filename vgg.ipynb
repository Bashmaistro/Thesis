{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: Tcga_lgg_olio_filtered\n",
      "Training on batch of size 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:03:18.226196: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 471859200 exceeds 10% of free system memory.\n",
      "2025-03-26 13:03:18.477105: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 471859200 exceeds 10% of free system memory.\n",
      "2025-03-26 13:03:18.988125: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 235929600 exceeds 10% of free system memory.\n",
      "2025-03-26 13:03:19.180050: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 235929600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 4s/step - accuracy: 0.2188 - loss: 1.4755"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:03:21.283386: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 471859200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.7295 - loss: 0.5109\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on final batch of size 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on final batch of size 36\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 433ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Processing folder: Tcga_lgg_astro_filtered\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.2870 - loss: 73.8800\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.7256e-04\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.0529e-06\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.9865 - loss: 0.0236\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.6898e-05\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.3617e-10\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.5092e-07\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.9453e-09\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.9527e-08\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.6007e-07\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.2057e-06\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.9600e-07\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on final batch of size 63\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.9892 - loss: 0.0771\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.6204e-07\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.2969e-09\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.0638e-08\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.6033e-05\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 4.3213e-09\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 9.9124e-09\n",
      "Training on final batch of size 156\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.4727e-09\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.8957e-08\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 9.9447e-05\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on final batch of size 164\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 5.9139e-10\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on final batch of size 32\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Processing folder: Tcga_gbm_filtered\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.2391 - loss: 120.8037\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0209\n",
      "Training on final batch of size 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Training on final batch of size 112\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.6998e-04\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0154\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.0962e-07\n",
      "Training on batch of size 200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0141\n",
      "Training on final batch of size 54\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.0742e-07\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pydicom.pixels import iter_pixels\n",
    "import cv2\n",
    "\n",
    "# VGG16 Modeli\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(240, 240, 3))\n",
    "vgg16.trainable = False\n",
    "\n",
    "# Üstüne yeni katmanlar ekleyelim\n",
    "model = models.Sequential()\n",
    "model.add(vgg16)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Klasörler\n",
    "folders = [\"Tcga_lgg_olio_filtered\", \"Tcga_lgg_astro_filtered\", \"Tcga_gbm_filtered\"]\n",
    "labels = {\"Tcga_lgg_olio_filtered\": 0, \"Tcga_lgg_astro_filtered\": 1, \"Tcga_gbm_filtered\": 2}\n",
    "\n",
    "# Eğitim Parametreleri\n",
    "batch_size = 200\n",
    "batch_data = []\n",
    "batch_labels = []\n",
    "\n",
    "# Eğitim Başlıyor\n",
    "for folder in folders:\n",
    "   print(f\"Processing folder: {folder}\")\n",
    "   label = labels[folder]\n",
    "\n",
    "   for file in os.listdir(folder):\n",
    "        \n",
    "      dcm_path = os.path.join(folder, file)\n",
    "           \n",
    "      dcm = pydicom.dcmread(dcm_path)\n",
    "            \n",
    "      for arr in iter_pixels(dcm_path):\n",
    "               \n",
    "            \n",
    "                \n",
    "               if arr.shape[0] == 256:\n",
    "                  start_x = (256 - 240) // 2\n",
    "                  start_y = (256 - 240) // 2\n",
    "\n",
    "\n",
    "                  arr = arr[start_y:start_y+240, start_x:start_x+240]\n",
    "               \n",
    "               # Normalize et ve batch'e ekle\n",
    "               arr = arr / 255.0\n",
    "               batch_data.append(arr)\n",
    "               batch_labels.append(label)\n",
    "\n",
    "               # Batch dolunca eğit\n",
    "               if len(batch_data) == batch_size:\n",
    "                  print(f\"Training on batch of size {len(batch_data)}\")\n",
    "                  nmpy_batch_data = np.array(batch_data)\n",
    "                  nmpy_batch_labels = to_categorical(batch_labels, num_classes=3)\n",
    "                  model.fit(nmpy_batch_data, nmpy_batch_labels, epochs=1, verbose=1)\n",
    "                  batch_data.clear()\n",
    "                  batch_labels.clear()\n",
    "                  \n",
    "\n",
    "      if batch_data:\n",
    "                  \n",
    "         print(f\"Training on final batch of size {len(batch_data)}\")\n",
    "         nmpy_batch_data = np.array(batch_data)\n",
    "         nmpy_batch_labels = to_categorical(batch_labels, num_classes=3)\n",
    "         model.fit(nmpy_batch_data, nmpy_batch_labels, epochs=1, verbose=1)     \n",
    "                  \n",
    "\n",
    "# Son kalan veriyi eğit\n",
    "\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "import cv2\n",
    "\n",
    "class DicomDataGenerator(Sequence):\n",
    "    def __init__(self, folders, labels, batch_size=32, input_size=(240, 240), num_classes=3, shuffle=True):\n",
    "        self.folders = folders\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Tüm dosya yollarını al\n",
    "        self.file_paths = []\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(folder)\n",
    "            files = os.listdir(folder_path)\n",
    "            self.file_paths.extend([(os.path.join(folder_path, f), labels[folder]) for f in files])\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.file_paths)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Kaç adımda bir epoch tamamlanacak\n",
    "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Batch için dosya yollarını seç\n",
    "        batch_paths = self.file_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for path, label in batch_paths:\n",
    "            # DICOM dosyasını yükle\n",
    "            dcm = pydicom.dcmread(path)\n",
    "            arr = dcm.pixel_array\n",
    "\n",
    "            # Görüntü boyutunu ayarla\n",
    "            arr = cv2.resize(arr, self.input_size)\n",
    "            \n",
    "            # Normalize et ve 3 kanallı hale getir (VGG16 için)\n",
    "            arr = arr.astype(np.float32) / np.max(arr)\n",
    "            arr = np.stack([arr] * 3, axis=-1)\n",
    "\n",
    "            batch_data.append(arr)\n",
    "            batch_labels.append(label)\n",
    "        \n",
    "        # Numpy array'e çevir\n",
    "        batch_data = np.array(batch_data)\n",
    "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Her epoch sonunda veriyi karıştır\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "import cv2\n",
    "\n",
    "class DicomDataGenerator(Sequence):\n",
    "    def __init__(self, folders, labels, batch_size=32, input_size=(240, 240), num_classes=3, shuffle=True):\n",
    "        self.folders = folders\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Tüm dosya yollarını al\n",
    "        self.file_paths = []\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(folder)\n",
    "            files = os.listdir(folder_path)\n",
    "            self.file_paths.extend([(os.path.join(folder_path, f), labels[folder]) for f in files])\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.file_paths)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Kaç adımda bir epoch tamamlanacak (Toplam slice sayısına bağlı)\n",
    "        total_slices = sum(pydicom.dcmread(path).pixel_array.shape[0] for path, _ in self.file_paths)\n",
    "        return int(np.ceil(total_slices / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        slice_count = 0\n",
    "        for path, label in self.file_paths:\n",
    "            # DICOM dosyasını yükle\n",
    "            dcm = pydicom.dcmread(path)\n",
    "            slices = dcm.pixel_array  # Shape: (slice, 256, 256, 3)\n",
    "            \n",
    "            for i in range(slices.shape[0]):\n",
    "                arr = slices[i]\n",
    "\n",
    "                # Görüntü boyutunu ayarla\n",
    "                arr = cv2.resize(arr, self.input_size)\n",
    "\n",
    "                # Normalize et ve doğru şekle getir\n",
    "                arr = arr.astype(np.float32) / np.max(arr)\n",
    "\n",
    "                # Eğer zaten 3 kanallıysa, direkt ekle; değilse çoğalt\n",
    "                if arr.shape[-1] != 3:\n",
    "                    arr = np.stack([arr] * 3, axis=-1)\n",
    "\n",
    "                batch_data.append(arr)\n",
    "                batch_labels.append(label)\n",
    "                slice_count += 1\n",
    "\n",
    "                # Batch dolduğunda çık\n",
    "                if slice_count >= self.batch_size:\n",
    "                    break\n",
    "\n",
    "            # Batch dolduysa dışarı çık\n",
    "            if slice_count >= self.batch_size:\n",
    "                break\n",
    "\n",
    "        # Numpy array'e çevir\n",
    "        batch_data = np.array(batch_data)\n",
    "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Her epoch sonunda veriyi karıştır\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3061s\u001b[0m 3s/step - accuracy: 0.8318 - loss: 11.1750\n",
      "Epoch 2/10\n",
      "\u001b[1m952/952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3253s\u001b[0m 3s/step - accuracy: 0.8194 - loss: 2.2326\n",
      "Epoch 3/10\n",
      "\u001b[1m291/952\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m41:44\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0592"
     ]
    }
   ],
   "source": [
    "# Klasörler ve etiketler\n",
    "folders = [\"Tcga_lgg_olio_filtered\", \"Tcga_lgg_astro_filtered\", \"Tcga_gbm_filtered\"]\n",
    "labels = {\"Tcga_lgg_olio_filtered\": 0, \"Tcga_lgg_astro_filtered\": 1, \"Tcga_gbm_filtered\": 2}\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Learning rate'i baştan düşük ayarla\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data Generator oluştur\n",
    "train_generator = DicomDataGenerator(folders, labels, batch_size=32)\n",
    "\n",
    "# Modeli eğit\n",
    "model.fit(train_generator, epochs=10, verbose=1)\n",
    "\n",
    "print(\"Eğitim tamamlandı!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
